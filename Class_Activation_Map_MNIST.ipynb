{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93df1a6",
   "metadata": {},
   "source": [
    "This work implements a simple class activation map (CAM) of a model trained on the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). This will show what parts of the image the model was paying attention to when deciding the class of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfde02d",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd28ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version:  2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"tf version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464925c8",
   "metadata": {},
   "source": [
    "# Data download and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c74d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 22:08:47.664835: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-10-15 22:08:47.665549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# set up \"download\" to True when first time download\n",
    "splits, info = tfds.load('fashion_mnist', with_info=True, split=['train', 'test'], \n",
    "                         data_dir='./MNIST', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92380671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf8a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (28, 28, 1)\n",
      "numbers of trainning example:  60000\n",
      "numbers of test example:  10000\n"
     ]
    }
   ],
   "source": [
    "image_shape = info.features['image'].shape\n",
    "train_num_examples = len(train_data)\n",
    "test_num_examples = len(test_data)\n",
    "\n",
    "print('image shape: ', str(image_shape))\n",
    "print('numbers of trainning example: ', train_num_examples)\n",
    "print('numbers of test example: ', test_num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72185b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image normalization and formating\n",
    "def format_image(data):\n",
    "    image = data['image']\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.\n",
    "    \n",
    "    return (image, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdeeaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display a sample image in the train data\n",
    "def show_sample_img():\n",
    "    for sample in train_data.take(1):\n",
    "        plt.imshow(sample['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03082b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 22:10:21.458957: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-10-15 22:10:21.500274: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_sample_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358f7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the batch dataset \n",
    "batch_size = 32\n",
    "train_dataset = train_data.map(format_image).shuffle(buffer_size=1024).batch(batch_size).prefetch(-1)\n",
    "test_dataset = test_data.map(format_image).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75d8c5",
   "metadata": {},
   "source": [
    "# Define a sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d278950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(inputs):\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='SAME')(inputs)\n",
    "    # pooling layer with a stride of 2 will reduce the image dimensions by half\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # pass through more convolutions with increasing filters\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='SAME')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='SAME')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='SAME')(x)\n",
    "    # use global average pooling to take into account lesser intensity pixels\n",
    "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
    "    \n",
    "    # output class probabilities\n",
    "    output = tf.keras.layers.Dense(10, 'softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d23514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,442\n",
      "Trainable params: 98,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(image_shape)\n",
    "model = sample_model(inputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962911eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ff9c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 22:12:46.049740: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.5846 - acc: 0.7844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 22:13:01.942701: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.5845 - acc: 0.7844 - val_loss: 0.4269 - val_acc: 0.8412\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.3621 - acc: 0.8673 - val_loss: 0.3516 - val_acc: 0.8716\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.3078 - acc: 0.8861 - val_loss: 0.3652 - val_acc: 0.8634\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.2728 - acc: 0.8990 - val_loss: 0.2776 - val_acc: 0.9027\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.2468 - acc: 0.9091 - val_loss: 0.2951 - val_acc: 0.8969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1630609d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "train_step = train_num_examples // batch_size\n",
    "validation_step = test_num_examples // batch_size\n",
    "EPOCHs = 5\n",
    "model.fit(train_dataset, epochs=EPOCHs, validation_data=test_dataset, steps_per_epoch=train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd41aa",
   "metadata": {},
   "source": [
    "## Generate the Class Activation Map\n",
    "\n",
    "To generate the class activation map, we want to get the features detected in the last convolution layer and see which ones are most active when generating the output probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f75fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_3\n",
      "global_average_pooling2d\n",
      "dense\n"
     ]
    }
   ],
   "source": [
    "# final convolution layer\n",
    "print(model.layers[-3].name)\n",
    "\n",
    "# global average pooling layer\n",
    "print(model.layers[-2].name)\n",
    "\n",
    "# output of the classifier\n",
    "print(model.layers[-1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c0518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,442\n",
      "Trainable params: 98,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cam_model = tf.keras.Model(inputs=model.input,outputs=[model.layers[-3].output, model.layers[-1].output])\n",
    "cam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a93f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "features shape:  (10000, 3, 3, 128)\n",
      "results shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# get the features and results of the test images using the newly created model\n",
    "features, results = cam_model.predict(test_dataset)\n",
    "X_test = [x['image'] for x in list(test_data)]\n",
    "# shape of the features\n",
    "print(\"features shape: \", features.shape)\n",
    "print(\"results shape\", results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bebb255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 128 feature weights and 10 classes.\n"
     ]
    }
   ],
   "source": [
    "last_dense_layer = model.layers[-1]\n",
    "# get the weights list. index 0 contains the weights, index 1 contains the biases\n",
    "gap_weights_l = last_dense_layer.get_weights()\n",
    "\n",
    "# Store the weights\n",
    "gap_weights = gap_weights_l[0]\n",
    "\n",
    "print(f\"There are {gap_weights.shape[0]} feature weights and {gap_weights.shape[1]} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e01e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam(image_index):\n",
    "    '''displays the class activation map of a particular image'''\n",
    "\n",
    "    # takes the features of the chosen image\n",
    "    features_for_img = features[image_index,:,:,:]\n",
    "\n",
    "    # get the class with the highest output probability\n",
    "    prediction = np.argmax(results[image_index])\n",
    "\n",
    "    # get the gap weights at the predicted class\n",
    "    class_activation_weights = gap_weights[:,prediction]\n",
    "\n",
    "    # upsample the features to the image's original size (28 x 28)\n",
    "    class_activation_features = sp.ndimage.zoom(features_for_img, (28/3, 28/3, 1), order=2)\n",
    "\n",
    "    # compute the intensity of each feature in the CAM\n",
    "    cam_output  = np.dot(class_activation_features,class_activation_weights)\n",
    "    \n",
    "#     print('Predicted Class = ' +str(prediction)+ ', Probability = ' + str(results[image_index][prediction]))\n",
    "    \n",
    "    # show the upsampled image\n",
    "    plt.imshow(np.squeeze(X_test[image_index],-1), alpha=0.5)\n",
    "    \n",
    "    # strongly classified (95% probability) images will be in green, else red\n",
    "    if results[image_index][prediction]>0.95:\n",
    "        cmap_str = 'Greens'\n",
    "    else:\n",
    "        cmap_str = 'Reds'\n",
    "\n",
    "    # overlay the cam output\n",
    "    plt.imshow(cam_output, cmap=cmap_str, alpha=0.5)\n",
    "    plt.xlabel('Probability: ' + str(results[image_index][prediction]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "048f5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_maps(desired_class, num_maps, save_img=False):\n",
    "    '''\n",
    "    goes through the first 10,000 test images and generates CAMs \n",
    "    for the first `num_maps`(int) of the `desired_class`(int)\n",
    "    '''\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    if desired_class < 10:\n",
    "        print(\"please choose a class less than 10\")\n",
    "\n",
    "    # go through the first 10000 images\n",
    "    fig = plt.figure()\n",
    "    for i in range(0,10000):\n",
    "        # break if we already displayed the specified number of maps\n",
    "        if counter == num_maps*num_maps:\n",
    "            break\n",
    "\n",
    "        # images that match the class will be shown\n",
    "        if np.argmax(results[i]) == desired_class:\n",
    "            counter += 1\n",
    "            ax = fig.add_subplot(num_maps, num_maps, counter)\n",
    "            show_cam(i)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    if save_img:\n",
    "        plt.savefig('mnist.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06503d6e",
   "metadata": {},
   "source": [
    "## show the CAM, where darker area implies the more intension the model focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "049cb392",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2810926482.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [30], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    show_maps(desired_class=7, num_maps=2, True)\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "show_maps(desired_class=7, num_maps=2, save_img=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bdba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
